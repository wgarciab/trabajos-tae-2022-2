---
title: "Análisis de resultados escolares con reducción de la dimensionalidad y agrupamiento"
author:
  - Amilder Stewin Ospina Tobon
  - Daniel Torres Aguirre
  - Wilmar Andres Garcia Bedoya
date: '2022-09-06'
output: html_document
---

# Introducción
En este informe, examinaremos una base de datos de universidades de estados unidos, proporcionada por el Departamento de Educación de los Estados Unidos. Nuestro objetivo será realizar un agrupamiento de estas universidades, para luego caracterizar cada grupo, y determinar qué hace que un grupo sea mejor que otro.

# Carga y limpieza de datos
Inicialmente, cargamos todos los datos. La base de datos contiene 7804 observaciones de 1725 variables.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(readr)
colleges <- read_csv("exercises-cluster-analysis-exercise-2/CollegeScorecard.csv", na = c("NULL", "PrivacySuppressed"))
dictionary <- read_csv("exercises-cluster-analysis-exercise-2/CollegeScorecardDataDictionary-09-12-2015.csv")
dim(colleges)
#str(colleges)
#summary(colleges)
head(colleges)
```

Quitamos las columnas nulas

```{r}
sum(is.na(colleges$mn_earn_wne_p8))
colleges <- Filter(function(x)!all(is.na(x)), colleges)
dim(colleges)
sapply(colleges, function(x) sum(is.na(x)))
```

Seleccionamos variables relevantes

```{r}
library(dplyr)
college_subset <- colleges %>% 
    select(OPEID, INSTNM, CITY, STABBR, ZIP, 
           CONTROL, DISTANCEONLY, CITY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN,SAT_AVG, PCTFLOAN, UGDS, NPT4_PUB, NPT4_PRIV)
```

Las variables seleccionados fueron las siguientes

```{r}
dictionary %>% filter(`VARIABLE NAME` %in% names(college_subset)) %>% select(`NAME OF DATA ELEMENT`)
```

Nulos en cada columna

```{r}
sapply(college_subset, function(x) sum(is.na(x)))
```
Reemplazamos nulos por la mediana

```{r}
library(plyr)
library(dplyr)
median_values <- colleges[,6:16] %>% 
    summarise_each(funs(median(., na.rm = TRUE)))

median_values

college_subset[6:16] <- college_subset[6:16] %>%
    mutate_each(funs(if_else(is.na(.), median(., na.rm = TRUE), .)))

```

```{r}
sapply(college_subset, function(x) sum(is.na(x)))
```

# PCA
```{r}
college_subset2 <- college_subset[6:16]
college_subset2 <- scale(college_subset2,center = TRUE,scale = TRUE)
college_pca <- prcomp(college_subset2)
summary(college_pca)
```
Con las primeras 7 variables se explica un 94% de la varianza total.

# Clustering

## Seleccion del numero de clusters

```{r}
library(broom)
library(tidyr)
library(dplyr)
set.seed(1234)
kclusts <- data.frame(k=4:20) %>% 
    group_by(k) %>% 
    do(kclust = kmeans(college_subset2, .$k))

clusters <- kclusts %>% group_by(k) %>% do(tidy(.$kclust[[1]]))
assignments <- kclusts %>% group_by(k) %>% do(augment(.$kclust[[1]], college_subset2))
clusterings <- kclusts %>% group_by(k) %>% do(glance(.$kclust[[1]]))

library(ggfortify)
ggplot(clusterings, aes(k, tot.withinss)) +
    geom_line(color = "blue", alpha = 0.5, size = 2) +
    geom_point(size = 0.8)
```

De acuerdo al metodo de elbow, el numero ideal de clusters es 7.

## k-means

```{r}
set.seed(1234)
autoplot(kmeans(college_subset2, 7), data = college_subset2, size = 3, alpha = 0.5) +
    ggtitle("K-Means Clustering of College Scorecard Data") +
    theme(legend.position="none")
my_kmeans <- kmeans(college_subset2, 7)
```

## Examinando los clusters...

En el cluster 2 hay muchas universidades de belleza y cosmetología

```{r}
colleges <- colleges %>% 
    mutate(cluster = my_kmeans$cluster) 

college_subset <- college_subset %>% 
    mutate(cluster = my_kmeans$cluster) 

colleges %>%
    filter(cluster == 1) %>%
    select(INSTNM)

cluster1 <- college_subset %>%
    filter(cluster == 1) %>%
    select(OPEID, INSTNM, CITY, STABBR, ZIP, 
           CONTROL, DISTANCEONLY, CITY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

summary(cluster1)
hist(cluster1$TUITFTE)
write.csv(cluster1,"cluster1.csv", row.names = FALSE)
```


# Referencias

https://rpubs.com/juliasilge/207156
https://rpubs.com/juliasilge/216478

