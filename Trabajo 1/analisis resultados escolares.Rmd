---
title: "Análisis de resultados escolares con reducción de la dimensionalidad y agrupamiento"
author:
  - Amilder Stewin Ospina Tobon
  - Daniel Torres Aguirre
  - Wilmar Andres Garcia Bedoya
date: '2022-09-06'
output: html_document
---

# Introducción
En este informe, examinaremos una base de datos de universidades de estados unidos, proporcionada por el Departamento de Educación de los Estados Unidos. Nuestro objetivo será realizar un agrupamiento de estas universidades, para luego caracterizar cada grupo, y determinar qué hace que un grupo sea mejor que otro.

# Carga y limpieza de datos
Inicialmente, cargamos todos los datos. La base de datos contiene 7804 observaciones de 1725 variables.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(readr)
colleges <- read_csv("exercises-cluster-analysis-exercise-2/CollegeScorecard.csv", na = c("NULL", "PrivacySuppressed"))
dictionary <- read_csv("exercises-cluster-analysis-exercise-2/CollegeScorecardDataDictionary-09-12-2015.csv")
dim(colleges)
#str(colleges)
#summary(colleges)
```
Podemos examinar las primeras 6 observaciones para hacernos una idea de la estructura de la base de datos

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
#kable(head(colleges))
DT::datatable(head(colleges))
```

Lo que haremos a continuación será quitar las columnas nulas, es decir, aquellas variables para las cuales no hay información. De esta forma, nuestra base de datos se redujo de 1725 a 551 columnas.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#sum(is.na(colleges$mn_earn_wne_p8))
colleges <- Filter(function(x)!all(is.na(x)), colleges)
dim(colleges)
#sapply(colleges, function(x) sum(is.na(x)))
```

Seleccionamos variables relevantes para nuestro problema, y las guardamos en un nuevo dataframe que contiene unicamente esas variables.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
college_subset <- colleges %>% 
    select(OPEID, INSTNM, CITY, STABBR, ZIP, 
           CONTROL, DISTANCEONLY, CITY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN,SAT_AVG, PCTFLOAN, UGDS, NPT4_PUB, NPT4_PRIV)
```

Las variables seleccionados fueron las siguientes

```{r, echo=FALSE, message=FALSE, warning=FALSE}
dictionary %>% filter(`VARIABLE NAME` %in% names(college_subset)) %>% select(`NAME OF DATA ELEMENT`)
```

Veamos cuantos nulos hay en cada columna

```{r, echo=FALSE, message=FALSE, warning=FALSE}
sapply(college_subset, function(x) sum(is.na(x)))
```

Reemplazamos los valores nulos de cada columna por la mediana de los demas valores existentes en esa columna.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(plyr)
library(dplyr)

college_subset2 <- college_subset[6:16]

median_values <- college_subset2 %>% 
    summarise_each(funs(median(., na.rm = TRUE)))

#median_values

college_subset2 <- college_subset2 %>%
    mutate_each(funs(if_else(is.na(.), median(., na.rm = TRUE), .)))

```

De esta manera, el número de nulos en todas las columnas queda en cero.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
sapply(college_subset2, function(x) sum(is.na(x)))
```

# PCA (Análisis de componentes principales)

Con las primeras 9 variables se explica un 92.9% de la varianza total.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
college_subset2 <- scale(college_subset2,center = TRUE,scale = TRUE)
college_pca <- prcomp(college_subset2)
summary(college_pca)
```

# Clustering

## Seleccion del numero de clusters

Para seleccionar el número adecuado de clusters, usaremos el método de Elbow. Veamos la curva generada:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(broom)
library(tidyr)
library(dplyr)
set.seed(1234)
kclusts <- data.frame(k=4:20) %>% 
    group_by(k) %>% 
    do(kclust = kmeans(college_subset2, .$k))

clusters <- kclusts %>% group_by(k) %>% do(tidy(.$kclust[[1]]))
assignments <- kclusts %>% group_by(k) %>% do(augment(.$kclust[[1]], college_subset2))
clusterings <- kclusts %>% group_by(k) %>% do(glance(.$kclust[[1]]))

library(ggfortify)
ggplot(clusterings, aes(k, tot.withinss)) +
    geom_line(color = "blue", alpha = 0.5, size = 2) +
    geom_point(size = 0.8)
```

De acuerdo al metodo de Elbow, el número ideal de clusters es 12.

## Agrupamiento con K-means

Para realizar el agrupamiento, utilizaremos k-means con 12 clústers. El resultado del agrupamiento de puede apreciar en la siguiente figura

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1234)
autoplot(kmeans(college_subset2, 12), data = college_subset2, size = 3, alpha = 0.5) +
    ggtitle("K-Means Clustering of College Scorecard Data") +
    theme(legend.position="none")

my_kmeans <- kmeans(college_subset2, 12)
```

## Caracterización de los clusters

En el cluster ? hay muchas universidades de belleza y cosmetología

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#colleges <- colleges %>% 
#    mutate(cluster = my_kmeans$cluster) 

college_subset2 = data.frame(college_subset2)
college_subset2 <- college_subset2 %>% 
    mutate(cluster = my_kmeans$cluster) 

#colleges %>%
#    filter(cluster == 1) %>%
#    select(INSTNM)

cluster1 <- college_subset2 %>%
    filter(cluster == 1) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster2 <- college_subset2 %>%
    filter(cluster == 2) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster3 <- college_subset2 %>%
    filter(cluster == 3) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster4 <- college_subset2 %>%
    filter(cluster == 4) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster5 <- college_subset2 %>%
    filter(cluster == 5) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster6 <- college_subset2 %>%
    filter(cluster == 6) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster7 <- college_subset2 %>%
    filter(cluster == 7) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster8 <- college_subset2 %>%
    filter(cluster == 8) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster9 <- college_subset2 %>%
    filter(cluster == 9) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster10 <- college_subset2 %>%
    filter(cluster == 10) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster11 <- college_subset2 %>%
    filter(cluster == 11) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)

cluster12 <- college_subset2 %>%
    filter(cluster == 12) %>%
    select(
           CONTROL, DISTANCEONLY, TUITFTE, ADM_RATE_ALL,
           C150_4, DEBT_MDN, NPT4_PUB, NPT4_PRIV, DEBT_MDN,SAT_AVG,
           PCTFLOAN, UGDS)


summary(cluster1)
hist(cluster1$TUITFTE)
write.csv(cluster1,"cluster1.csv", row.names = FALSE)
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(fmsb)

#normalize <- function(x) {
#  return((x - min(x)) / (max(x) - min(x)))
#}

cluster1_means = t(colMeans(cluster1[3:11]))
cluster2_means = t(colMeans(cluster2[3:11]))
cluster3_means = t(colMeans(cluster3[3:11]))
cluster4_means = t(colMeans(cluster4[3:11]))
cluster5_means = t(colMeans(cluster5[3:11]))
cluster6_means = t(colMeans(cluster6[3:11]))
cluster7_means = t(colMeans(cluster7[3:11]))
cluster8_means = t(colMeans(cluster8[3:11]))
cluster9_means = t(colMeans(cluster9[3:11]))
cluster10_means = t(colMeans(cluster10[3:11]))
cluster11_means = t(colMeans(cluster11[3:11]))
cluster12_means = t(colMeans(cluster12[3:11]))


df = data.frame()
df = rbind(df, cluster1_means)
df = rbind(df, cluster2_means)
df = rbind(df, cluster3_means)
df = rbind(df, cluster4_means)
df = rbind(df, cluster5_means)
df = rbind(df, cluster6_means)
df = rbind(df, cluster7_means)
df = rbind(df, cluster8_means)
df = rbind(df, cluster9_means)
df = rbind(df, cluster10_means)
df = rbind(df, cluster11_means)
df = rbind(df, cluster12_means)

df_norm <- as.data.frame(apply(df[, 1:9], 2, function(x) (x - min(x))/(max(x)-min(x))))

df_norm <- rbind(rep(max(df_norm), 9), rep(0, 9), df_norm)

radarchart(df_norm, 
           seg = 9,  # Number of axis segments
           title = "Clusters de universidades",
           pcol = 1:12,
           plwd = 2)

legend(x=1.15, 
       y=1.35, 
       legend = rownames(df_norm[-c(1,2),]-2),
       bty = "n", pch=20 , col = 1:12, cex = 1.05, pt.cex = 1.5)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
#para visualizar cada cluster por separado, cambiar el siguiente numero:
cluster_number = 3
df_cluster_mean = df_norm[cluster_number,]
df_cluster_mean <- rbind(rep(max(df_cluster_mean), 9), rep(0, 9), df_cluster_mean)

radarchart(df_cluster_mean, 
           seg = 9,  # Number of axis segments
           title = "Clusters de universidades",
           pcol = 1,
           plwd = 2)

```


# Referencias

https://rpubs.com/juliasilge/207156
https://rpubs.com/juliasilge/216478

